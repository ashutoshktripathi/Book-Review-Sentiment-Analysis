{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classifier_XLNETLarge_WordLenght_LTE_250.ipynb",
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CQpiOpeyjm7"
      },
      "source": [
        "**This Notebook trains a sentiment classification model on a book review dataset**\n",
        "\n",
        "The model is specially designed for reviews with word count lenght less than 250.\n",
        "\n",
        "Here we have used Transformers library and will be usin XLNET transformner with fine tuning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9mrnvaJGWUe"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-WEoV1TIkT-"
      },
      "source": [
        "pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnMOWE1fGUMu"
      },
      "source": [
        "# install the required libraries\n",
        "import os\n",
        "import math\n",
        "\n",
        "import torch\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import AdamW, XLNetTokenizer, XLNetModel, XLNetLMHeadModel, XLNetConfig\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm, trange\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7N0kK-Aww-Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oqk4t77G8kQ"
      },
      "source": [
        "# check the GPU availability\n",
        "print(\"GPU Available: {}\".format(torch.cuda.is_available()))\n",
        "n_gpu = torch.cuda.device_count()\n",
        "print(\"Number of GPU Available: {}\".format(n_gpu))\n",
        "print(\"GPU: {}\".format(torch.cuda.get_device_name(0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCegnlDtGxjK"
      },
      "source": [
        "# mount the google drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnsBbSU0HBg2"
      },
      "source": [
        "def lenCategory(x):\n",
        "    if x == 1:\n",
        "        return \"1\"\n",
        "    \n",
        "    if x == 2:\n",
        "        return \"2\"\n",
        "    \n",
        "    if x == 3:\n",
        "        return \"3\"\n",
        "    \n",
        "    if x <= 5:\n",
        "        return \"4-5\"\n",
        "    \n",
        "    if x <= 10:\n",
        "        return \"6-10\"\n",
        "    \n",
        "    if x <= 25:\n",
        "        return \"11-25\"\n",
        "    \n",
        "    if x <= 50:\n",
        "        return \"26-50\"\n",
        "    \n",
        "    if x <= 100:\n",
        "        return \"51-100\"\n",
        "    \n",
        "    if x <= 250:\n",
        "        return \"101-250\"\n",
        "    \n",
        "    if x <= 500:\n",
        "        return \"251-500\"\n",
        "    \n",
        "    if x <= 1000:\n",
        "        return \"501-1000\"\n",
        "    \n",
        "    if x <= 2500:\n",
        "        return \"1001-2500\"\n",
        "    \n",
        "    if x <= 5000:\n",
        "        return \"2501-5000\"\n",
        "    \n",
        "    return '5000+'\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0iAEWJmHXgP"
      },
      "source": [
        "# custom function to load the data\n",
        "def loadData(path):\n",
        "    data = pd.read_csv(path)\n",
        "    data = data.drop_duplicates(\"review\")\n",
        "    data[\"len\"] = data.apply(lambda x : len(str(x[\"review\"]).split()) , axis = 1)\n",
        "    data[\"lencat\"] = data.apply(lambda x : lenCategory(x[\"len\"]) , axis = 1)\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqaw07xBHZv5"
      },
      "source": [
        "train = loadData('/content/gdrive/MyDrive/Toptal/train_data.csv')\n",
        "holdout = loadData('/content/gdrive/MyDrive/Toptal/holdout_data.csv')\n",
        "\n",
        "train = train.dropna()\n",
        "holdout = holdout.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDV3GEJBHb_R"
      },
      "source": [
        "# select the data with wordcount less than equal to 250\n",
        "train = train[~train.lencat.isin(['501-1000','1001-2500', '2501-5000',\"251-500\"])]\n",
        "holdout = holdout[~holdout.lencat.isin(['501-1000','1001-2500', '2501-5000',\"251-500\"])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duwJP-q-H8nS"
      },
      "source": [
        "def plot_sentence_embeddings_length(text_list, tokenizer):\n",
        "    tokenized_texts = list(map(lambda t: tokenizer.tokenize(t), text_list))\n",
        "    tokenized_texts_len = list(map(lambda t: len(t), tokenized_texts))\n",
        "    fig, ax = plt.subplots(figsize=(8, 5));\n",
        "    ax.hist(tokenized_texts_len, bins=40);\n",
        "    ax.set_xlabel(\"Length of Comment Embeddings\");\n",
        "    ax.set_ylabel(\"Number of Comments\");\n",
        "    return\n",
        "    \n",
        "# load the XLNET tokenizer based on XLNET LARGE CASED\n",
        "tokenizer = XLNetTokenizer.from_pretrained('xlnet-large-cased', do_lower_case=True)\n",
        "\n",
        "# get the review text\n",
        "train_text_list = train[\"review\"].values\n",
        "test_text_list = holdout[\"review\"].values\n",
        "\n",
        "plot_sentence_embeddings_length(train_text_list, tokenizer)\n",
        "\n",
        "plot_sentence_embeddings_length(test_text_list, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4xTAMPoIchd"
      },
      "source": [
        "def tokenize_inputs(text_list, tokenizer, num_embeddings=512):\n",
        "    \"\"\"\n",
        "    Tokenizes the input text input into ids. Appends the appropriate special\n",
        "    characters to the end of the text to denote end of sentence. Truncate or pad\n",
        "    the appropriate sequence length.\n",
        "    \"\"\"\n",
        "    # tokenize the text, then truncate sequence to the desired length minus 2 for\n",
        "    # the 2 special characters\n",
        "    tokenized_texts = list(map(lambda t: tokenizer.tokenize(t)[:num_embeddings-2], text_list))\n",
        "    # convert tokenized text into numeric ids for the appropriate LM\n",
        "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "    # append special token \"<s>\" and </s> to end of sentence\n",
        "    input_ids = [tokenizer.build_inputs_with_special_tokens(x) for x in input_ids]\n",
        "    # pad sequences\n",
        "    input_ids = pad_sequences(input_ids, maxlen=num_embeddings, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "    return input_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_wwlXJeg2Dj"
      },
      "source": [
        "# create input id tokens\n",
        "\n",
        "train_input_ids = tokenize_inputs(train_text_list, tokenizer)\n",
        "train_input_ids.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZApAlETKXj1"
      },
      "source": [
        "test_input_ids = tokenize_inputs(test_text_list, tokenizer)\n",
        "test_input_ids.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9hn9X9_K2N5"
      },
      "source": [
        "def create_attn_masks(input_ids):\n",
        "    \"\"\"\n",
        "    Create attention masks to tell model whether attention should be applied to\n",
        "    the input id tokens. Do not want to perform attention on padding tokens.\n",
        "    \"\"\"\n",
        "    # Create attention masks\n",
        "    attention_masks = []\n",
        "\n",
        "    # Create a mask of 1s for each token followed by 0s for padding\n",
        "    for seq in input_ids:\n",
        "        seq_mask = [float(i>0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "    return attention_masks\n",
        "    \n",
        "train_attention_masks = create_attn_masks(train_input_ids)\n",
        "test_attention_masks = create_attn_masks(test_input_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsWTA_7RK58C"
      },
      "source": [
        "# add input ids and attention masks to the dataframe\n",
        "train[\"features\"] = train_input_ids.tolist()\n",
        "train[\"masks\"] = train_attention_masks\n",
        "\n",
        "holdout[\"features\"] = test_input_ids.tolist()\n",
        "holdout[\"masks\"] = test_attention_masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EymajY-rLTnZ"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMFOdgBILZUY"
      },
      "source": [
        "holdout.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnSdHaWFLbzk"
      },
      "source": [
        "\n",
        "# train valid split\n",
        "train, valid = train_test_split(train, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vjjavp70LpK-"
      },
      "source": [
        "X_train = train[\"features\"].values.tolist()\n",
        "X_valid = valid[\"features\"].values.tolist()\n",
        "\n",
        "train_masks = train[\"masks\"].values.tolist()\n",
        "valid_masks = valid[\"masks\"].values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64kiMjWiM_aQ"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "one_hot = OneHotEncoder(sparse=False).fit(\n",
        "  train.sentiment.to_numpy().reshape(-1, 1)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt1anq18NaUI"
      },
      "source": [
        "Y_train = one_hot.transform(train.sentiment.to_numpy().reshape(-1, 1))\n",
        "Y_valid = one_hot.transform(valid.sentiment.to_numpy().reshape(-1, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OoknBL8Nbnr"
      },
      "source": [
        "Y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUekPb7PNzIq"
      },
      "source": [
        "# Convert all of our input ids and attention masks into \n",
        "# torch tensors, the required datatype for our model\n",
        "\n",
        "X_train = torch.tensor(X_train)\n",
        "X_valid = torch.tensor(X_valid)\n",
        "\n",
        "Y_train = torch.tensor(Y_train, dtype=torch.float32)\n",
        "Y_valid = torch.tensor(Y_valid, dtype=torch.float32)\n",
        "\n",
        "train_masks = torch.tensor(train_masks, dtype=torch.long)\n",
        "valid_masks = torch.tensor(valid_masks, dtype=torch.long)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_vTYH_9N7yY"
      },
      "source": [
        "# Select a batch size for training\n",
        "batch_size = 8\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on \n",
        "# memory during training because, unlike a for loop, \n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "\n",
        "train_data = TensorDataset(X_train, train_masks, Y_train)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data,\\\n",
        "                              sampler=train_sampler,\\\n",
        "                              batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(X_valid, valid_masks, Y_valid)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data,\\\n",
        "                                   sampler=validation_sampler,\\\n",
        "                                   batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLDM3_TsOGUk"
      },
      "source": [
        "def train(model, num_epochs,\\\n",
        "          optimizer,\\\n",
        "          train_dataloader, valid_dataloader,\\\n",
        "          model_save_path,\\\n",
        "          train_loss_set=[], valid_loss_set = [],\\\n",
        "          lowest_eval_loss=None, start_epoch=0,\\\n",
        "          device=\"cpu\"\n",
        "          ):\n",
        "  \"\"\"\n",
        "  Train the model and save the model with the lowest validation loss\n",
        "  \"\"\"\n",
        "\n",
        "  model.to(device)\n",
        "\n",
        "  # trange is a tqdm wrapper around the normal python range\n",
        "  for i in trange(num_epochs, desc=\"Epoch\"):\n",
        "    # if continue training from saved model\n",
        "    actual_epoch = start_epoch + i\n",
        "\n",
        "    # Training\n",
        "\n",
        "    # Set our model to training mode (as opposed to evaluation mode)\n",
        "    model.train()\n",
        "\n",
        "    # Tracking variables\n",
        "    tr_loss = 0\n",
        "    num_train_samples = 0\n",
        "\n",
        "    # Train the data for one epoch\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "      # Add batch to GPU\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      # Unpack the inputs from our dataloader\n",
        "      b_input_ids, b_input_mask, b_labels = batch\n",
        "      # Clear out the gradients (by default they accumulate)\n",
        "      optimizer.zero_grad()\n",
        "      # Forward pass\n",
        "      loss = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
        "      # store train loss\n",
        "      tr_loss += loss.item()\n",
        "      num_train_samples += b_labels.size(0)\n",
        "      # Backward pass\n",
        "      loss.backward()\n",
        "      # Update parameters and take a step using the computed gradient\n",
        "      optimizer.step()\n",
        "      #scheduler.step()\n",
        "\n",
        "    # Update tracking variables\n",
        "    epoch_train_loss = tr_loss/num_train_samples\n",
        "    train_loss_set.append(epoch_train_loss)\n",
        "\n",
        "    print(\"Train loss: {}\".format(epoch_train_loss))\n",
        "\n",
        "    # Validation\n",
        "\n",
        "    # Put model in evaluation mode to evaluate loss on the validation set\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss = 0\n",
        "    num_eval_samples = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in valid_dataloader:\n",
        "      # Add batch to GPU\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      # Unpack the inputs from our dataloader\n",
        "      b_input_ids, b_input_mask, b_labels = batch\n",
        "      # Telling the model not to compute or store gradients,\n",
        "      # saving memory and speeding up validation\n",
        "      with torch.no_grad():\n",
        "        # Forward pass, calculate validation loss\n",
        "        loss = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
        "        # store valid loss\n",
        "        eval_loss += loss.item()\n",
        "        num_eval_samples += b_labels.size(0)\n",
        "\n",
        "    epoch_eval_loss = eval_loss/num_eval_samples\n",
        "    valid_loss_set.append(epoch_eval_loss)\n",
        "\n",
        "    print(\"Valid loss: {}\".format(epoch_eval_loss))\n",
        "\n",
        "    if lowest_eval_loss == None:\n",
        "      lowest_eval_loss = epoch_eval_loss\n",
        "      # save model\n",
        "      save_model(model, model_save_path, actual_epoch,\\\n",
        "                 lowest_eval_loss, train_loss_set, valid_loss_set)\n",
        "    else:\n",
        "      if epoch_eval_loss < lowest_eval_loss:\n",
        "        lowest_eval_loss = epoch_eval_loss\n",
        "        # save model\n",
        "        save_model(model, model_save_path, actual_epoch,\\\n",
        "                   lowest_eval_loss, train_loss_set, valid_loss_set)\n",
        "    print(\"\\n\")\n",
        "\n",
        "  return model, train_loss_set, valid_loss_set\n",
        "\n",
        "\n",
        "def save_model(model, save_path, epochs, lowest_eval_loss, train_loss_hist, valid_loss_hist):\n",
        "  \"\"\"\n",
        "  Save the model to the path directory provided\n",
        "  \"\"\"\n",
        "  model_to_save = model.module if hasattr(model, 'module') else model\n",
        "  checkpoint = {'epochs': epochs, \\\n",
        "                'lowest_eval_loss': lowest_eval_loss,\\\n",
        "                'state_dict': model_to_save.state_dict(),\\\n",
        "                'train_loss_hist': train_loss_hist,\\\n",
        "                'valid_loss_hist': valid_loss_hist\n",
        "               }\n",
        "  torch.save(checkpoint, save_path)\n",
        "  print(\"Saving model at epoch {} with validation loss of {}\".format(epochs,\\\n",
        "                                                                     lowest_eval_loss))\n",
        "  return\n",
        "  \n",
        "def load_model(save_path):\n",
        "  \"\"\"\n",
        "  Load the model from the path directory provided\n",
        "  \"\"\"\n",
        "  checkpoint = torch.load(save_path)\n",
        "  model_state_dict = checkpoint['state_dict']\n",
        "  model = XLNetForMultiLabelSequenceClassification(num_labels=model_state_dict[\"classifier.weight\"].size()[0])\n",
        "  model.load_state_dict(model_state_dict)\n",
        "\n",
        "  epochs = checkpoint[\"epochs\"]\n",
        "  lowest_eval_loss = checkpoint[\"lowest_eval_loss\"]\n",
        "  train_loss_hist = checkpoint[\"train_loss_hist\"]\n",
        "  valid_loss_hist = checkpoint[\"valid_loss_hist\"]\n",
        "  \n",
        "  return model, epochs, lowest_eval_loss, train_loss_hist, valid_loss_hist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfW4HHPcOwPX"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TI5HqJMcOy4f"
      },
      "source": [
        "#config = XLNetConfig()\n",
        "        \n",
        "class XLNetForMultiLabelSequenceClassification(torch.nn.Module):\n",
        "  \n",
        "  def __init__(self, num_labels=2):\n",
        "    super(XLNetForMultiLabelSequenceClassification, self).__init__()\n",
        "    self.num_labels = num_labels\n",
        "    self.xlnet = XLNetModel.from_pretrained('xlnet-base-cased')\n",
        "    self.classifier = torch.nn.Linear(768, num_labels)\n",
        "\n",
        "    torch.nn.init.xavier_normal_(self.classifier.weight)\n",
        "\n",
        "  def forward(self, input_ids, token_type_ids=None,\\\n",
        "              attention_mask=None, labels=None):\n",
        "    # last hidden layer\n",
        "    last_hidden_state = self.xlnet(input_ids=input_ids,\\\n",
        "                                   attention_mask=attention_mask,\\\n",
        "                                   token_type_ids=token_type_ids)\n",
        "    # pool the outputs into a mean vector\n",
        "    mean_last_hidden_state = self.pool_hidden_state(last_hidden_state)\n",
        "    logits = self.classifier(mean_last_hidden_state)\n",
        "        \n",
        "    if labels is not None:\n",
        "      loss_fct = BCEWithLogitsLoss()\n",
        "      loss = loss_fct(logits.view(-1, self.num_labels),\\\n",
        "                      labels.view(-1, self.num_labels))\n",
        "      return loss\n",
        "    else:\n",
        "      return logits\n",
        "    \n",
        "  def freeze_xlnet_decoder(self):\n",
        "    \"\"\"\n",
        "    Freeze XLNet weight parameters. They will not be updated during training.\n",
        "    \"\"\"\n",
        "    for param in self.xlnet.parameters():\n",
        "      param.requires_grad = False\n",
        "    \n",
        "  def unfreeze_xlnet_decoder(self):\n",
        "    \"\"\"\n",
        "    Unfreeze XLNet weight parameters. They will be updated during training.\n",
        "    \"\"\"\n",
        "    for param in self.xlnet.parameters():\n",
        "      param.requires_grad = True\n",
        "    \n",
        "  def pool_hidden_state(self, last_hidden_state):\n",
        "    \"\"\"\n",
        "    Pool the output vectors into a single mean vector \n",
        "    \"\"\"\n",
        "    last_hidden_state = last_hidden_state[0]\n",
        "    mean_last_hidden_state = torch.mean(last_hidden_state, 1)\n",
        "    return mean_last_hidden_state\n",
        "    \n",
        "model = XLNetForMultiLabelSequenceClassification(num_labels=3)\n",
        "#model = torch.nn.DataParallel(model)\n",
        "#model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt-P2nVPO3cE"
      },
      "source": [
        "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01, correct_bias=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4-KRcihO90k"
      },
      "source": [
        "# num_epochs=10\n",
        "\n",
        "# # cwd = os.getcwd()\n",
        "# model_save_path = output_model_file = \"/content/gdrive/MyDrive/Toptal/xlnet_11-25_aug_balanced.bin\"\n",
        "# model, train_loss_set, valid_loss_set = train(model=model,\\\n",
        "#                                               num_epochs=num_epochs,\\\n",
        "#                                               optimizer=optimizer,\\\n",
        "#                                               train_dataloader=train_dataloader,\\\n",
        "#                                               valid_dataloader=validation_dataloader,\\\n",
        "#                                               model_save_path=model_save_path,\\\n",
        "#                                               device=\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlAuikqT0Suc"
      },
      "source": [
        "\"\"\"Epoch:   0%|          | 0/10 [00:00<?, ?it/s]Train loss: 0.019482453157987224\n",
        "Valid loss: 0.017420511438132023\n",
        "Epoch:  10%|█         | 1/10 [7:13:50<65:04:38, 26030.91s/it]Saving model at epoch 0 with validation loss of 0.017420511438132023\n",
        "\n",
        "\n",
        "Train loss: 0.01679534113878288\n",
        "Epoch:  20%|██        | 2/10 [14:27:13<57:48:33, 26014.17s/it]Valid loss: 0.017582947640206906\n",
        "\n",
        "\n",
        "Epoch:  20%|██        | 2/10 [16:06:24<64:25:38, 28992.35s/it]\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_qha-KUv7cf"
      },
      "source": [
        "def generate_predictions(model, df, num_labels, device=\"cpu\", batch_size=32):\n",
        "  num_iter = math.ceil(df.shape[0]/batch_size)\n",
        "  \n",
        "  pred_probs = np.array([]).reshape(0, num_labels)\n",
        "  \n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  \n",
        "  for i in range(num_iter):\n",
        "    df_subset = df.iloc[i*batch_size:(i+1)*batch_size,:]\n",
        "    X = df_subset[\"features\"].values.tolist()\n",
        "    masks = df_subset[\"masks\"].values.tolist()\n",
        "    X = torch.tensor(X)\n",
        "    masks = torch.tensor(masks, dtype=torch.long)\n",
        "    X = X.to(device)\n",
        "    masks = masks.to(device)\n",
        "    with torch.no_grad():\n",
        "      logits = model(input_ids=X, attention_mask=masks)\n",
        "      logits = logits.sigmoid().detach().cpu().numpy()\n",
        "      pred_probs = np.vstack([pred_probs, logits])\n",
        "  \n",
        "  return pred_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQEzrEkW0gzR"
      },
      "source": [
        "# load the saved model \n",
        "model, start_epoch, lowest_eval_loss, train_loss_hist, valid_loss_hist = load_model(\"/content/gdrive/MyDrive/Toptal/xlnet_11-25_aug_balanced.bin\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2nGjaDQ0g4R"
      },
      "source": [
        "num_labels = 3\n",
        "pred_probs = generate_predictions(model, holdout, num_labels, device=\"cuda\", batch_size=32)\n",
        "preds = np.argmax(pred_probs, axis = 1)\n",
        "ytrue = holdout.sentiment.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yF_guXb0g60"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix, classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouG2ctS2X1nK"
      },
      "source": [
        "accuracy_score(ytrue,preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SC15YJAYLjH"
      },
      "source": [
        "print(classification_report(ytrue,preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iwO3YT4loi3"
      },
      "source": [
        "'''\n",
        "\n",
        "0s\n",
        "print(classification_report(ytrue,preds))\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.76      0.73      0.74      9295\n",
        "           1       0.63      0.47      0.54     14963\n",
        "           2       0.95      0.98      0.97    133307\n",
        "\n",
        "    accuracy                           0.92    157565\n",
        "   macro avg       0.78      0.73      0.75    157565\n",
        "weighted avg       0.91      0.92      0.91    157565\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl0CK8xjDBKZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}